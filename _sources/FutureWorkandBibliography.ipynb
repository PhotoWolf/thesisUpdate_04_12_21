{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "parental-election",
   "metadata": {},
   "source": [
    "# Addendum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-absence",
   "metadata": {},
   "source": [
    "## Future Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-amazon",
   "metadata": {},
   "source": [
    "- [3] use the first eight Laplacian eigenmodes as positional embeddings; they find that this greatly improves GCN performance on a few clustering datasets. Computing multiple eigenmodes is not terribly cheap, and it may not always be beneficial. We'd ideally like for our models to be able to approximate these modes from scratch *if* useful to the task at hand. Unfortunatly, standard GCN architectures are not well-equipped to learn anything besides the dominant eigenvector. To address this, we have begun to experiment with a UNet-style architecture, where, instead of operating at different resolutions, we aggregate over different modes. Each subsequent layer is periodically orthogonalized against the output of the previous; think of it as an Arnoldi-GCN hybrid. \n",
    "\n",
    "- We'd like to do some more experimentation with OrthNorm. It could potentially be quite useful for minimizing oversmoothing in node classification tasks, and there is a wealth of GCN normalization literature with which we can compare. \n",
    "\n",
    "- Eventually, we need to test our various models, normalization schemes, etc. on some proper benchmarks. MoleculeNet contains a series of graph datasets for organic and quantum chemistry [20]. Unliked most benchmarks (Cora, Citeseer, etc.), MoleculeNet is predominantly topological, and some of the datasets do not even possess node features aside from relative positions in $R^{3}$. As far as structure is concerned, it is the ideal \"real-world\" case to test any techniques we may develop. [3] also introduced some synthetic datasets which may be of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-classics",
   "metadata": {},
   "source": [
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-harvey",
   "metadata": {},
   "source": [
    "1. Gilmer, J., Schoenholz, S., Riley, P.F., Vinyals, O., & Dahl, G.E. (2017). Neural Message Passing for Quantum Chemistry. ArXiv, abs/1704.01212.\n",
    "2. Hu, Z., Fan, C., Chen, T., Chang, K., & Sun, Y. (2019). Pre-Training Graph Neural Networks for Generic Structural Feature Extraction. ArXiv, abs/1905.13728.\n",
    "3. Dwivedi, V.P., Joshi, C.K., Laurent, T., Bengio, Y., & Bresson, X. (2020). Benchmarking Graph Neural Networks. ArXiv, abs/2003.00982.\n",
    "4. Chen, Hongxu & Yin, Hongzhi & Chen, Tong & Hung, Nguyen & Peng, Wen-Chih & Li, Xue. (2019). Exploiting Centrality Information with Graph Convolutions for Network Representation Learning. 590-601. 10.1109/ICDE.2019.00059. \n",
    "5. Mendonça, M.R., Barreto, A., & Ziviani, A. (2021). Approximating Network Centrality Measures Using Node Embedding and Machine Learning. IEEE Transactions on Network Science and Engineering, 8, 220-230.\n",
    "6. Grando, F., Granville, L., & Lamb, L. (2019). Machine Learning in Network Centrality Measures. ACM Computing Surveys (CSUR), 51, 1 - 32.\n",
    "7. Morris, C., Ritzert, M., Fey, M., Hamilton, W.L., Lenssen, J.E., Rattan, G., & Grohe, M. (2019). Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks. AAAI.\n",
    "8. Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Liò, P., & Bengio, Y. (2018). Graph Attention Networks. ArXiv, abs/1710.10903.\n",
    "9. Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M., & Solomon, J. (2019). Dynamic Graph CNN for Learning on Point Clouds. ACM Transactions on Graphics (TOG), 38, 1 - 12.\n",
    "10. Ioffe, S., & Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. ArXiv, abs/1502.03167.\n",
    "11. Maas, A.L. (2013). Rectifier Nonlinearities Improve Neural Network Acoustic Models.\n",
    "12. Karrer, B., & Newman, M. (2011). Stochastic blockmodels and community structure in networks. Physical review. E, Statistical, nonlinear, and soft matter physics, 83 1 Pt 2, 016107 .\n",
    "13. Katz, L. A new status index derived from sociometric analysis. Psychometrika 18, 39–43 (1953).\n",
    "14. Chen, D., Lin, Y., Li, W., Li, P., Zhou, J., & Sun, X. (2020). Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View. AAAI.\n",
    "15. Kipf, T., & Welling, M. (2017). Semi-Supervised Classification with Graph Convolutional Networks. ArXiv, abs/1609.02907.\n",
    "16. Zhao, L., & Akoglu, L. (2020). PairNorm: Tackling Oversmoothing in GNNs. ArXiv, abs/1909.12223.\n",
    "17. Wu, F., Zhang, T., Souza, A., Fifty, C., Yu, T., & Weinberger, K.Q. (2019). Simplifying Graph Convolutional Networks. ArXiv, abs/1902.07153.\n",
    "18. Abu-El-Haija, S., Perozzi, B., Kapoor, A., Harutyunyan, H., Alipourfard, N., Lerman, K., Steeg, G.V., & Galstyan, A. (2019). MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing. ICML.\n",
    "19. Brandes, U. (2001). A faster algorithm for betweenness centrality. The Journal of Mathematical Sociology, 25, 163 - 177.\n",
    "20. Wu, Z., Ramsundar, B., Feinberg, E., Gomes, J., Geniesse, C., Pappu, A.S., Leswing, K., & Pande, V. (2018). MoleculeNet: a benchmark for molecular machine learning† †Electronic supplementary information (ESI) available. See DOI: 10.1039/c7sc02664a. Chemical Science, 9, 513 - 530.\n",
    "21. Cai, T., Luo, S., Xu, K., He, D., Liu, T., & Wang, L. (2020). GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training. ArXiv, abs/2009.03294."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
