{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bottom-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Introduction import *\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-growth",
   "metadata": {},
   "source": [
    "# Normalization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-session",
   "metadata": {},
   "source": [
    "Within the literature, the typical solution to oversmoothing is to apply some type of normalization. To this end, we propose orthogonalizing out vector $\\vec{q}$ (essentially a \"best guess\" for $v_{1}$) from $X^{l}$ and then renormalizing each column. We incorporate additional scale parameters $0\\leq{}s_{1}\\leq{}1$ and $s_{2}$.\n",
    "\n",
    "$$X_{i}^{l} = X_{i}^{l} - s_{1}\\frac{X_{i}^{l}\\cdot{}\\vec{q}}{||\\vec{q}||_{2}^{2}}\\vec{q}$$\n",
    "$$X_{i}^{l} = s_{2}\\frac{X_{i}^{l}}{||X_{i}^{l}||_{2}}$$\n",
    "\n",
    "If $s_{1}=1$ and $\\vec{q}=\\vec{1}$, this is equivalent to PairNorm [16]. Define the smoothness of feature $i$ at GCN layer $l$ as\n",
    "\n",
    "$$S = \\frac{1}{E[X^{l} - E[X^{l}]]}$$\n",
    "\n",
    "PairNorm aims to minimize global smoothness by subtracting out the mean feature vector and renormalizing. \n",
    "\n",
    "$$X_{i}^{l} = X_{i}^{l} - E[X_{i}^{l}]$$\n",
    "$$X_{i}^{l} = s \\frac{X_{i}^{l}}{||X_{i}^{l}||_{2}}$$\n",
    "\n",
    "We evaluate our normalization scheme with $\\vec{q} = \\frac{1}{\\vec{d}_{degree}}$ and compare against both PairNorm and GraphSizeNorm [3]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-trigger",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "minute-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_graphs = 3000\n",
    "d = []\n",
    "for _ in range(num_graphs):\n",
    "    n = torch.randint(50,100,(5,))\n",
    "    p = 1/(50*n) + (49/(50*n)) * torch.rand((5,5))\n",
    "    p = .5 * (p + p.T)\n",
    "    x,edges = torch.ones((n.sum(),1)),torch_geometric.utils.remove_isolated_nodes(torch_geometric.utils.stochastic_blockmodel_graph(n,p))[0]\n",
    "    adj = torch_sparse.SparseTensor(row=edges[0],col=edges[1])\n",
    "\n",
    "    d.append(torch_geometric.data.Data(x=x[:adj.size(0)],edge_index = edges))\n",
    "\n",
    "for idx,G in enumerate(d):\n",
    "    G.edge_weight = torch.ones(G.edge_index[0].shape)\n",
    "    adj = torch_sparse.SparseTensor(row=G.edge_index[0],col=G.edge_index[1],value=G.edge_weight)\n",
    "    v = 1/(1.01*torch.norm(torch.eig(adj.to_dense())[0],dim=1).max())\n",
    "    y = torch.sum(torch.inverse(torch.eye(adj.size(0)) - v*adj.to_dense().T) - torch.eye(adj.size(0)),dim=1)\n",
    "    G.y = y\n",
    "    d[idx] = G\n",
    "    \n",
    "train,test = d[:2000],d[2000::]\n",
    "train_loader = torch_geometric.data.DataLoader(train,batch_size=200,shuffle=True)\n",
    "test_loader = torch_geometric.data.DataLoader(test,batch_size=200,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-insertion",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caring-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_MAP(X,edge_index,edge_weights):\n",
    "    X = X/torch.norm(X,dim=1)[:,None]\n",
    "    cosine = 1 - torch.sum(X[edge_index[0]] * X[edge_index[1]],dim=1)\n",
    "    return 1/edge_weights.sum() * (edge_weights * cosine).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "centered-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthNormL2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OrthNormL2,self).__init__()\n",
    "        self.s1 = torch.nn.Parameter(torch.ones(1))\n",
    "        self.s2 = torch.nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self,X,edge_index,edge_weight,batch):\n",
    "        n = 1/torch_geometric.utils.degree(edge_index[0])\n",
    "        n_norm = torch_scatter.scatter_sum(n**2,batch,dim=0)\n",
    "        \n",
    "        alpha = torch_scatter.scatter_sum(X * n[:,None],batch,dim=0)/(n_norm)[:,None]\n",
    "        \n",
    "        X = X - torch.sigmoid(self.s1) * (alpha)[batch] * n[batch][:,None]\n",
    "        X = self.s2 * X/(torch_scatter.scatter_sum(X**2,batch,dim=0).sqrt())[batch]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adaptive-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel(torch.nn.Module):\n",
    "    def __init__(self,in_channels,int_channels,out_channels,depth,norm,p=2):\n",
    "        super(DummyModel,self).__init__()\n",
    "        self.start = torch.nn.Linear(in_channels,int_channels)\n",
    "        self.intermediate = torch.nn.ModuleList([torch.nn.ModuleList([torch.nn.Linear(int_channels,int_channels),\\\n",
    "                                                                      torch.nn.Linear(int_channels,int_channels)])\\\n",
    "                                                 for _ in range(depth)])\n",
    "        self.norm = torch.nn.ModuleList([norm() for _ in range(depth)])\n",
    "        self.finish = torch.nn.Linear(int_channels,out_channels)\n",
    "\n",
    "    def forward(self,X,edge_index,edge_weight,batch):\n",
    "        X = self.start(X)\n",
    "        for idx,m in enumerate(self.intermediate):\n",
    "            X = X + m[0](X) + torch_scatter.scatter_sum(edge_weight[:,None] * m[1](X)[edge_index[1]], edge_index[0],dim=0)\n",
    "            X = torch.nn.LeakyReLU()(self.norm[idx](X,edge_index,edge_weight,batch))\n",
    "            if torch.isnan(X).any(): raise ValueError\n",
    "        return self.finish(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-polish",
   "metadata": {},
   "source": [
    "## OrthNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_results = []\n",
    "model_mad = []\n",
    "\n",
    "torch.manual_seed(0)\n",
    "for k in [2,4,8,16,32,64]:\n",
    "    torch.manual_seed(0)\n",
    "    graph = DummyModel(1,32,1,k,OrthNormL2).cuda()\n",
    "    \n",
    "    graph_results.append(train_loop(graph,train_loader,test_loader,150,lr=1e-1))\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    MAD = torch.zeros(k+1)\n",
    "    for idx,data in enumerate(test_loader):\n",
    "        X = data.x.cuda()\n",
    "        edge_index,edge_weight = data.edge_index.cuda(),data.edge_weight.cuda()\n",
    "        batch = data.batch.cuda()\n",
    "\n",
    "        graph.eval()\n",
    "        X = graph.start(X)\n",
    "        MAD[0] += batched_MAP(X,edge_index,edge_weight).mean().item()\n",
    "\n",
    "        for jdx,m in enumerate(graph.intermediate):\n",
    "            X = X + m[0](X) + torch_scatter.scatter_sum(m[1](X)[edge_index[1]], edge_index[0],dim=0)\n",
    "            X = torch.nn.LeakyReLU()(graph.norm[jdx](X,edge_index,edge_weight,batch))\n",
    "            MAD[jdx+1] += batched_MAP(X,edge_index,edge_weight).mean().item()\n",
    "\n",
    "    model_mad.append(MAD/(idx+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-contemporary",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "for i in range(6):\n",
    "  plt.semilogy(graph_results[i][0])\n",
    "\n",
    "plt.title('Train Error')\n",
    "plt.ylabel('L1 Error')\n",
    "plt.xlabel('Iterations')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "for i in range(6):\n",
    "  plt.semilogy(graph_results[i][1])\n",
    "\n",
    "plt.title('Test Error')\n",
    "plt.xlabel('Iterations')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "for idx,alpha in enumerate([2,4,8,16,32,64]):\n",
    "  plt.semilogy(graph_results[idx][2],label=alpha)\n",
    "plt.title('Ranking Error')\n",
    "plt.ylabel('Avg. Displacement')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,d in enumerate(model_mad):\n",
    "  plt.plot(d,label='GraphConv')\n",
    "  plt.xlabel('Layer')\n",
    "  plt.ylabel('Mean Average Distance')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-imagination",
   "metadata": {},
   "source": [
    "## GraphSizeNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSizeNorm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GraphSizeNorm,self).__init__()\n",
    "        self.norm = torch_geometric.nn.GraphSizeNorm()\n",
    "    def forward(self,X,edge_index,edge_weight,batch):\n",
    "        return self.norm(X,batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_results = []\n",
    "model_mad = []\n",
    "\n",
    "torch.manual_seed(0)\n",
    "for k in tqdm([2,4,8,16,32,64]):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    graph = DummyModel(1,32,1,k,GraphSizeNorm).cuda()\n",
    "    \n",
    "    graph_results.append(train_loop(graph,train_loader,test_loader,150,lr=1e-1))\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    MAD = torch.zeros(k+1)\n",
    "    for idx,data in enumerate(test_loader):\n",
    "        X = data.x.cuda()\n",
    "        edge_index,edge_weight = data.edge_index.cuda(),data.edge_weight.cuda()\n",
    "        batch = data.batch.cuda()\n",
    "\n",
    "        graph.eval()\n",
    "        X = graph.start(X)\n",
    "        MAD[0] += batched_MAP(X,edge_index,edge_weight).mean().item()\n",
    "\n",
    "        for jdx,m in enumerate(graph.intermediate):\n",
    "            X = X + m[0](X) + torch_scatter.scatter_sum(m[1](X)[edge_index[1]], edge_index[0],dim=0)\n",
    "            X = torch.nn.LeakyReLU()(graph.norm[jdx](X,edge_index,edge_weight,batch))\n",
    "            MAD[jdx+1] += batched_MAP(X,edge_index,edge_weight).mean().item()\n",
    "        \n",
    "    model_mad.append(MAD/(idx+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-register",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "for i in range(6):\n",
    "  plt.semilogy(graph_results[i][0])\n",
    "\n",
    "plt.title('Train Error')\n",
    "plt.ylabel('L1 Error')\n",
    "plt.xlabel('Iterations')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "for i in range(6):\n",
    "  plt.semilogy(graph_results[i][1])\n",
    "\n",
    "plt.title('Test Error')\n",
    "plt.xlabel('Iterations')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "for idx,alpha in enumerate([2,4,8,16,32,64]):\n",
    "  plt.semilogy(graph_results[idx][2],label=alpha)\n",
    "plt.title('Ranking Error')\n",
    "plt.ylabel('Avg. Displacement')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,d in enumerate(model_mad):\n",
    "  plt.plot(d,label='GraphConv')\n",
    "  plt.xlabel('Layer')\n",
    "  plt.ylabel('Mean Average Distance')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-perfume",
   "metadata": {},
   "source": [
    "## PairNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairNorm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PairNorm,self).__init__()\n",
    "        self.norm = torch_geometric.nn.PairNorm(scale_individually=True)\n",
    "    def forward(self,X,edge_index,edge_weight,batch):\n",
    "        return self.norm(X,batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_results = []\n",
    "model_mad = []\n",
    "\n",
    "for k in [1,2,4,8,16,32,64]:\n",
    "    torch.manual_seed(0)\n",
    "    graph = DummyModel(1,32,1,k,PairNorm).cuda()\n",
    "    \n",
    "    graph_results.append(train_loop(graph,train_loader,test_loader,150,lr=1e-1))\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    MAD = torch.zeros(k+1)\n",
    "    for idx,data in enumerate(test_loader):\n",
    "        X = data.x.cuda()\n",
    "        edge_index,edge_weight = data.edge_index.cuda(),data.edge_weight.cuda()\n",
    "        batch = data.batch.cuda()\n",
    "\n",
    "        graph.eval()\n",
    "        X = graph.start(X)\n",
    "        MAD[0] += batched_MAP(X,edge_index,edge_weight).mean().item()\n",
    "\n",
    "        for jdx,m in enumerate(graph.intermediate):\n",
    "            X = X + m[0](X) + torch_scatter.scatter_sum(m[1](X)[edge_index[1]], edge_index[0],dim=0)\n",
    "            X = torch.nn.LeakyReLU()(graph.norm[jdx](X,edge_index,edge_weight,batch))\n",
    "            MAD[jdx+1] += batched_MAP(X,edge_index,edge_weight).mean().item()\n",
    "\n",
    "        \n",
    "    model_mad.append(MAD/(idx+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-keeping",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "for i in range(7):\n",
    "  plt.semilogy(graph_results[i][0])\n",
    "\n",
    "plt.title('Train Error')\n",
    "plt.ylabel('L1 Error')\n",
    "plt.xlabel('Iterations')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "for i in range(7):\n",
    "  plt.semilogy(graph_results[i][1])\n",
    "\n",
    "plt.title('Test Error')\n",
    "plt.xlabel('Iterations')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "for idx,alpha in enumerate([1,2,4,8,16,32,64]):\n",
    "  plt.semilogy(graph_results[idx][2],label=alpha)\n",
    "plt.title('Ranking Error')\n",
    "plt.ylabel('Avg. Displacement')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,d in enumerate(model_mad):\n",
    "  plt.plot(d,label='GraphConv')\n",
    "  plt.xlabel('Layer')\n",
    "  plt.ylabel('Mean Average Distance')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-islam",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-birth",
   "metadata": {},
   "source": [
    "Our new normalization scheme, which we are calling OrthNorm (i.e Orthagonal Normalization, get it?), improves upon the unnormalized case, even without having fully converged. It surpasses PairNorm by around an order of magnitude given the same depth and number of epochs. We suspect this is because $d_{Katz}$ is close enough to $v_{1}$, and so PairNorm reduces the smoothness *too* much. Indeed, PairNorm returns uniform MAP values, whereas those of OrthNorm exhibit a great deal of variance. This is due to $s_{1}$ regulating the magnitude of the orthogonalization; without it, MAP consistently hovers around $.20$, and the loss is about $1e^{-3}$ larger. \n",
    "\n",
    "As for GraphSizeNorm, there is certainly some benefit, but it does still converge to a larger value than OrthNorm (which isn't even convergent yet). Oddly enough, while the MAP is otherwise quite low, GraphSizeNorm spikes in the last few layers for both the $l=32$ and $l=64$ models. We do not have an explanation for why this occurs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
